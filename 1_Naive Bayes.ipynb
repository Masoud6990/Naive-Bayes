{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "grader_version": "1"
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nimport string",
      "metadata": {
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "import w1_unittest",
      "metadata": {},
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "dataframe_emails = pd.read_csv('emails.csv')\ndataframe_emails.head()",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Subject: naturally irresistible your corporate...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Subject: 4 color printing special  request add...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Subject: do not have money , get software cds ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  spam\n",
              "0  Subject: naturally irresistible your corporate...     1\n",
              "1  Subject: the stock trading gunslinger  fanny i...     1\n",
              "2  Subject: unbelievable new homes made easy  im ...     1\n",
              "3  Subject: 4 color printing special  request add...     1\n",
              "4  Subject: do not have money , get software cds ...     1"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "#exploring the dataset\nprint(f\"Number of emails: {len(dataframe_emails)}\")\nprint(f\"Proportion of spam emails: {dataframe_emails.spam.sum()/len(dataframe_emails):.4f}\")\nprint(f\"Proportion of ham emails: {1-dataframe_emails.spam.sum()/len(dataframe_emails):.4f}\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of emails: 5728\n",
            "Proportion of spam emails: 0.2388\n",
            "Proportion of ham emails: 0.7612\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "def preprocess_emails(df):\n    # Shuffles the dataset\n    df = df.sample(frac = 1, ignore_index = True, random_state = 42)\n    # Removes the \"Subject:\" string, which comprises the first 9 characters of each email. Also, convert it to a numpy array.\n    X = df.text.apply(lambda x: x[9:]).to_numpy()\n    # Convert the labels to numpy array\n    Y = df.spam.to_numpy()\n    return X, Y",
      "metadata": {},
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "X, Y = preprocess_emails(dataframe_emails)",
      "metadata": {},
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": "def preprocess_text(X):\n    \"\"\"\n    Preprocesses a collection of text data by removing stopwords and punctuation.\n    \"\"\"\n    # Make a set with the stopwords and punctuation\n    stop = set(stopwords.words('english') + list(string.punctuation))\n\n    # The next lines will handle the case where a single email is passed instead of an array of emails.\n    if isinstance(X, str):\n        X = np.array([X])\n\n    # The result will be stored in a list\n    X_preprocessed = []\n\n    for i, email in enumerate(X):\n        email = np.array([i.lower() for i in word_tokenize(email) if i.lower() not in stop]).astype(X.dtype)\n        X_preprocessed.append(email)\n        \n    if len(X) == 1:\n        return X_preprocessed[0]\n    return X_preprocessed\n",
      "metadata": {},
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "source": "\nX_treated = preprocess_text(X)",
      "metadata": {},
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": "#Splitting data into train/test\nTRAIN_SIZE = int(0.80*len(X_treated)) # 80% of the samples will be used to train.\nX_train = X_treated[:TRAIN_SIZE]\nY_train = Y[:TRAIN_SIZE]\nX_test = X_treated[TRAIN_SIZE:]\nY_test = Y[TRAIN_SIZE:]",
      "metadata": {},
      "outputs": [],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "def get_word_frequency(X,Y):\n    \"\"\"\n    Calculate the frequency of each word in a set of emails categorized as spam (1) or not spam (0).\n    \n    \"\"\"\n    word_dict = {}\n\n    num_emails = len(X)\n\n    for i in range(num_emails):\n        email = X[i] \n        cls = Y[i] \n        email = set(email) \n        for word in email:\n            if word not in word_dict.keys():\n                word_dict[word] = {'spam': 1, 'ham': 1}\n            if cls == 0:    \n                word_dict[word]['ham'] += 1\n            if cls == 1:\n                word_dict[word]['spam'] += 1\n    \n    return word_dict",
      "metadata": {
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "# Building the word_frequency dictionary using the training set. \nword_frequency = get_word_frequency(X_train,Y_train)",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": "# Counting the spam and ham emails\nclass_frequency = {'ham': sum(Y_train == 0), 'spam': sum(Y_train == 1)}",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": "def prob_word_given_class(word, cls, word_frequency, class_frequency):\n    \"\"\"\n    Calculate the conditional probability of a given word occurring in a specific class.\n\n    \"\"\"\n    # Get the amount of times the word appears with the given class (class is stores in spam variable)\n    amount_word_and_class = word_frequency[word][cls]\n    p_word_given_class = amount_word_and_class/class_frequency[cls]\n    \n    return p_word_given_class",
      "metadata": {
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": "def prob_email_given_class(treated_email, cls, word_frequency, class_frequency):\n    \"\"\"\n    Calculate the probability of an email being of a certain class (e.g., spam or ham) based on treated email content.\n\n    \"\"\"\n\n    prob = 1\n\n    for word in treated_email:\n        \n        if word in word_frequency.keys(): \n\n            prob *= prob_word_given_class(word, cls, word_frequency = word_frequency, class_frequency = class_frequency)\n\n    return prob",
      "metadata": {
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "execution_count": 27
    },
    {
      "cell_type": "code",
      "source": "def naive_bayes(treated_email, word_frequency, class_frequency, return_likelihood = False):    \n    \"\"\"\n    Naive Bayes classifier for spam detection.\n\n    This function determines whether an email is likely to be spam (1) or not spam (0) using the Naive Bayes algorithm.\n    It relies on the conditional probabilities associated with the treated email being classified as spam or not spam,\n    along with the prior probabilities of spam and not spam classes. The ultimate classification is determined by comparing these calculated probabilities.\n\n    \"\"\"\n\n    prob_email_given_spam = prob_email_given_class(treated_email, 'spam', word_frequency, class_frequency)\n\n    prob_email_given_ham = prob_email_given_class(treated_email, 'ham', word_frequency, class_frequency)\n\n    p_spam = class_frequency['spam']/(class_frequency['spam']+class_frequency['ham'])\n    \n    p_ham = class_frequency['ham']/(class_frequency['spam']+class_frequency['ham'])\n\n    spam_likelihood = p_spam * prob_email_given_spam\n\n    ham_likelihood = p_ham * prob_email_given_ham\n    \n    if return_likelihood == True:\n        return (spam_likelihood, ham_likelihood)\n    \n    elif spam_likelihood >= ham_likelihood:\n        return 1\n    else:\n        return 0",
      "metadata": {
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": "def get_true_positives(Y_true, Y_pred):\n    \"\"\"\n    Calculate the number of true positive instances in binary classification.\n\n    \"\"\"\n    if len(Y_true) != len(Y_pred):\n        return \"Number of true labels and predict labels must match!\"\n    n = len(Y_true)\n    true_positives = 0\n    for i in range(n):\n        true_label_i = Y_true[i]\n        predicted_label_i = Y_pred[i]\n        if true_label_i == 1 and predicted_label_i == 1:\n            true_positives += 1\n    return true_positives\n        \ndef get_true_negatives(Y_true, Y_pred):\n    \"\"\"\n    Calculate the number of true negative instances in binary classification.\n\n    \"\"\"\n    if len(Y_true) != len(Y_pred):\n        return \"Number of true labels and predict labels must match!\"\n    n = len(Y_true)\n    true_negatives = 0\n    for i in range(n):\n        true_label_i = Y_true[i]\n        predicted_label_i = Y_pred[i]\n        if true_label_i == 0 and predicted_label_i == 0:\n            true_negatives += 1\n    return true_negatives\n        ",
      "metadata": {},
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "code",
      "source": "\nY_pred = []\nfor email in X_test:\n    prediction = naive_bayes(email, word_frequency, class_frequency)\n    Y_pred.append(prediction)\nprint(f\"Y_test and Y_pred matches in length? Answer: {len(Y_pred) == len(Y_test)}\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Y_test and Y_pred matches in length? Answer: True\n"
          ]
        }
      ],
      "execution_count": 34
    },
    {
      "cell_type": "code",
      "source": "true_positives = get_true_positives(Y_test, Y_pred)\ntrue_negatives = get_true_negatives(Y_test, Y_pred)\nprint(f\"The number of true positives is: {true_positives}\\nThe number of true negatives is: {true_negatives}\")\naccuracy = (true_positives + true_negatives)/len(Y_test)\nprint(f\"Accuracy is: {accuracy:.4f}\")",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of true positives is: 249\n",
            "The number of true negatives is: 723\n",
            "Accuracy is: 0.8482\n"
          ]
        }
      ],
      "execution_count": 37
    },
    {
      "cell_type": "code",
      "source": "print(f\"The example email has: {len(treated_email)} words in the product.\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "So the email you are investigating has $2657$ words! Let's compute the value $P(\\text{word} \\mid \\text{ham})$ for the first 3 words in the email:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "for i in range(3):\n    word = treated_email[i]\n    p_word_given_ham = prob_word_given_class(word, cls = 'ham', word_frequency = word_frequency, class_frequency = class_frequency)\n    print(f\"Word: {word}. P({word} | ham) = {p_word_given_ham}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Given that they are all probabilities, they are numbers between $0$ and $1$. So, the product being performed is a product of $2657$ numbers between $0$ and $1$. In the best-case scenario, where every word has a probability in the magnitude of $10^{-1}$ (similar to the first word in the example above), the resulting probability would be in the magnitude of $10^{-2657}$—a **very small number** that is challenging for any computer to handle with precision. Let's examine Python's limit on floating-point numbers (decimal numbers):",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import sys\n\nprint(sys.float_info)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "As you can see, the minimum float value has a magnitude of $10^{-308}$, significantly larger than $10^{-2657}$. Consequently, Python interprets the result of the product as $0$ at some point, leading to the loss of all information. In other words, the way your algorithm is currently written, past a certain length, all emails are being classified as spam. Given the nature of this issue, rooted in the very large product required by Naive Bayes, it is crucial to address the problem.\n\n#### 5.1.1 The Underflow Problem\n\nThe challenge you encounter is termed an **underflow problem**, indicating that you are dealing with exceedingly small numbers beyond the computer's precision. In this case, the root cause is the **very large product** involved in Naive Bayes calculations. Fortunately, there is a solution to this issue.\n\nRecall that in Naive Bayes, the specific values of probabilities are not critical since the algorithm solely **compares values**. This is why the denominators in the following equations have been disregarded:\n\n$$ P(\\text{spam} \\mid \\text{email}) = \\frac{P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam})}{P(\\text{email})} $$\n$$ P(\\text{ham} \\mid \\text{email}) = \\frac{P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})}{P(\\text{email}) } $$\n\nGiven that the goal is to identify the greater value between the two, and they share the same positive denominator, only the numerators matter. Specifically, the actual values of these two products:\n\n$$P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam})$$\n$$P(\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham})$$\n\nare irrelevant, as long as you can tell which one is larger than the other.\n\nIf there exists a function that can be applied to these quantities and **preserves the ordering**, then comparing the outputs of these values in such a function will determine the class with the maximum value (although the actual numeric value may differ). \n\nAny **strictly increasing function** possesses this property: it preserves the maximum **point**. Therefore, the idea is to find a **increasing function** that aids in handling the large product faced by the Naive Bayes algorithm. Can you think of one? Well, there is one: the $\\log$ function. As you may already know, $\\log$ can transform **products** into **sums**! Since $\\log$ is increasing, it preserves the maximum point. Therefore, you can compare the following quantities:\n\n$$\\log \\left(P(\\text{spam}) \\cdot P(\\text{email} \\mid \\text{spam}) \\right)$$\n$$\\log \\left(P (\\text{ham}) \\cdot P(\\text{email} \\mid \\text{ham}) \\right)$$\n\nAnd choose the maximum value among these new quantities. Denoting the class as either spam or ham:\n\n$$\\log \\left(P(\\text{class}) \\cdot P(\\text{email} \\mid \\text{class}) \\right) = \\log \\left(P(\\text{class}) \\right) + \\log \\left( P(\\text{email} \\mid \\text{class}) \\right)$$\n\nAnd\n\n$$\\log \\left( P(\\text{email} \\mid \\text{class}) \\right) = \\log  \\left(P(\\text{word}_1 \\mid \\text{class}) \\cdot P(\\text{word}_2 \\mid \\text{class}) \\cdots P(\\text{word}_n \\mid \\text{class}) \\right) = \\log  \\left(P(\\text{word}_1 \\mid \\text{class}) \\right) + \\log \\left(P(\\text{word}_2 \\mid \\text{class})\\right) + \\cdots + \\log \\left( P(\\text{word}_n \\mid \\text{class}) \\right) $$\n\nWith this approach, you have transformed a large product into a large summation, a significantly more numerically stable operation. Now, you will improve our functions with this new technique! You need to adjust two functions:\n\n- `prob_email_given_class` - replace the probability word product by the sum of the logs\n- `naive_bayes` - replace the product $P(\\text{class}) \\cdot P(\\text{email} \\mid \\text{class})$ by its respective sum of log.\n\nThe new functions will be called `log_prob_email_given_class` and `log_naive_bayes`.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def log_prob_email_given_class(treated_email, cls, word_frequency, class_frequency):\n    \"\"\"\n    Calculate the log probability of an email being of a certain class (e.g., spam or ham) based on treated email content.\n\n    Parameters:\n    - treated_email (list): A list of treated words in the email.\n    - cls (str): The class label ('spam' or 'ham')\n    \n\n    Returns:\n    - float: The log probability of the given email belonging to the specified class.\n    \"\"\"\n\n    # prob starts at 0 because it will be updated by summing it with the current log(P(word | class)) in every iteration\n    prob = 0\n\n    for word in treated_email: \n        # Only perform the computation for words that exist in the word frequency dictionary\n        if word in word_frequency.keys(): \n            # Update the prob by summing it with log(P(word | class))\n            prob += np.log(prob_word_given_class(word, cls,word_frequency, class_frequency))\n\n    return prob",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Consider an email with only one word, so it reduces to compute the value P(word | class) or log(P(word | class)).\none_word_email = ['schedule']\nword = one_word_email[0]\nprob_spam = prob_email_given_class(one_word_email, cls = 'spam',word_frequency = word_frequency, class_frequency = class_frequency)\nlog_prob_spam = log_prob_email_given_class(one_word_email, cls = 'spam',word_frequency = word_frequency, class_frequency = class_frequency)\nprint(f\"For word {word}:\\n\\tP({word} | spam) = {prob_spam}\\n\\tlog(P({word} | spam)) = {log_prob_spam}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Note that the $\\text{log}$ was capable of transforming a small number into a negative number with a good magnitude. Furthermore, now the algorithm is performing a sum instead of product.\n\nThe next code block implements the log_naive_bayes.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def log_naive_bayes(treated_email, word_frequency, class_frequency, return_likelihood = False):    \n    \"\"\"\n    Naive Bayes classifier for spam detection, comparing the log probabilities instead of the actual probabilities.\n\n    This function calculates the log probability of an email being spam (1) or ham (0)\n    based on the Naive Bayes algorithm. It uses the conditional probabilities of the\n    treated_email given spam and ham, as well as the prior probabilities of spam and ham\n    classes. The final decision is made by comparing the calculated probabilities.\n\n    Parameters:\n    - treated_email (list): A preprocessed representation of the input email.\n    - return_likelihood (bool): If true, it returns the log_likelihood of both spam and ham.\n\n    Returns:\n    - int: 1 if the email is classified as spam, 0 if classified as ham.\n    \"\"\"\n    \n    # Compute P(email | spam) with the new log function\n    log_prob_email_given_spam = log_prob_email_given_class(treated_email, cls = 'spam',word_frequency = word_frequency, class_frequency = class_frequency) \n\n    # Compute P(email | ham) with the function you defined just above\n    log_prob_email_given_ham = log_prob_email_given_class(treated_email, cls = 'ham',word_frequency = word_frequency, class_frequency = class_frequency) \n\n    # Compute P(spam) using the class_frequency dictionary and using the formula #spam emails / #total emails\n    p_spam = class_frequency['spam']/(class_frequency['ham'] + class_frequency['spam']) \n\n    # Compute P(ham) using the class_frequency dictionary and using the formula #ham emails / #total emails\n    p_ham = class_frequency['ham']/(class_frequency['ham'] + class_frequency['spam']) \n\n    # Compute the quantity log(P(spam)) + log(P(email | spam)), let's call it log_spam_likelihood\n    log_spam_likelihood = np.log(p_spam) + log_prob_email_given_spam \n\n    # Compute the quantity P(ham) * P(email | ham), let's call it ham_likelihood\n    log_ham_likelihood = np.log(p_ham) + log_prob_email_given_ham \n\n    # In case of passing return_likelihood = True, then return the desired tuple\n    if return_likelihood == True:\n        return (log_spam_likelihood, log_ham_likelihood)\n    \n    # Compares both values and choose the class corresponding to the higher value. \n    # As the logarithm is an increasing function, the class with the higher value retains this property.\n    if log_spam_likelihood >= log_ham_likelihood:\n        return 1\n    else:\n        return 0",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Revisiting the example from the beginning of the section, you will compute `log_spam_likelihood` and `log_ham_likelihood`",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "log_spam_likelihood, log_ham_likelihood = log_naive_bayes(treated_email,word_frequency = word_frequency, class_frequency = class_frequency,return_likelihood = True)\nprint(f\"log_spam_likelihood: {log_spam_likelihood}\\nlog_ham_likelihood: {log_ham_likelihood}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Great! Now there are two distinct non-zero numbers! Note the higher one is the `log_ham_likelihood`, therefore the `log_naive_bayes` function will correctly predict this email as ham:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(f\"The example email is labeled as: {Y[example_index]}\")\nprint(f\"Log Naive bayes model classifies it as: {log_naive_bayes(treated_email,word_frequency = word_frequency, class_frequency = class_frequency)}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "With this enhanced algorithm, the new accuracy is:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Let's get the predictions for the test set:\n\n# Create an empty list to store the predictions\nY_pred = []\n\n\n# Iterate over every email in the test set\nfor email in X_test:\n    # Perform prediction\n    prediction = log_naive_bayes(email,word_frequency = word_frequency, class_frequency = class_frequency)\n    # Add it to the list \n    Y_pred.append(prediction)\n\n# Get the number of true positives:\ntrue_positives = get_true_positives(Y_test, Y_pred)\n\n# Get the number of true negatives:\ntrue_negatives = get_true_negatives(Y_test, Y_pred)\n\nprint(f\"The number of true positives is: {true_positives}\\nThe number of true negatives is: {true_negatives}\")\n\n# Compute the accuracy by summing true negatives with true positives and dividing it by the total number of elements in the dataset. \n# Since both Y_pred and Y_test have the same length, it does not matter which one you use.\naccuracy = (true_positives + true_negatives)/len(Y_test)\n\nprint(f\"The accuracy is: {accuracy:.4f}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "This is a **huge** improvement! You've increased the model's accuracy from 84.82% to 99.21% in the test set! An increase of almost 17%. And you haven't touched the dataset, it was an improvement purely in the **math** behind it. Powerful, right?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "<a name=\"5.2\"></a>\n### 5.2 Enhancing model performance: Practical implementation with Naive Bayes\n\n#### 5.2.1 Introduction\n\nIn this section you will use both Naive Bayes models (with and without log) you've defined above to solve a problem:\n\nYou must develop a good spam detection model to run in a specific email software. The dataset you worked with in this assignment is the email base you have from this software. You must build a method to effectively protect users from receiving spam, **but you must avoid sending ham emails to the spam folder** since it might cause a user to lose important emails. On the other hand, it is not that concerning letting pass a couple of spam emails to the inbox folder. \n\n#### 5.2.2 Accuracy and its limitations\n\nRight now, what is the actual performance of the model you've developed thus far? The accuracy metric you defined above has some limitations, specially in this spam detection problem. You have seen in the beginning of the notebook that the proportion of spam emails in the dataset is 23.88%. So, if you create a rule to send **every email directly to inbox folder** it would correctly classify 76.12% of every email! So this pointless rule has an accuracy of 76.12%. \n\nTo try to properly answer this question, you can ask yourself two questions:\n\n- How many spam emails the algorithm correctly classifies as spam? They are called **true positives**.\n- How many **ham** emails the algorithm **mistakenly classifies** as spam? They are called **false positives**. **This is the important question you must look closer.**\n\nThe first question relates to a metric called [*recall*](https://en.wikipedia.org/wiki/Precision_and_recall). To answer the first question, you must count how many spam emails there exist in the dataset and count how many of them are correctly labeled as spam by the model (true positives). This is defined as the recall:\n\n$$\\text{recall} = \\frac{\\text{true positives (spam emails correctly labeled as spam)}}{\\text{every spam email}}$$\n\nAnother way you may see this metric being defined is by considering that a spam email will be either correctly labeled as spam (true positive) or mistakenly labeled as ham (false negative), so \n\n$$\\text{recall} =\\frac{\\text{true positives}}{\\text{true positives} + \\text{false negatives}}$$\n\nYou will now make the recall function.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_recall(Y_true, Y_pred):\n    \"\"\"\n    Calculate the recall for a binary classification task.\n\n    Parameters:\n    - Y_true (array-like): Ground truth labels.\n    - Y_pred (array-like): Predicted labels.\n\n    Returns:\n    - recall (float): The recall score, which is the ratio of true positives to the total number of actual positives.\n    \"\"\"\n    # Get the total number of spam emails. Since they are 1 in the data, it suffices summing all the values in the array Y.\n    total_number_spams = Y_test.sum()\n    # Get the true positives\n    true_positives = get_true_positives(Y_true, Y_pred)\n    \n    # Compute the recall\n    recall = true_positives/total_number_spams\n    return recall",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Use the Naive Bayes model (standard and log versions) to classify every email in the test dataset\nY_pred_naive_bayes = []\nY_pred_log_naive_bayes = []\n\nfor email in X_test:\n prediction = naive_bayes(email,word_frequency = word_frequency, class_frequency = class_frequency)\n log_prediction = log_naive_bayes(email,word_frequency = word_frequency, class_frequency = class_frequency)\n Y_pred_naive_bayes.append(prediction)\n Y_pred_log_naive_bayes.append(log_prediction)\n\n# Compute the recall for both models\nrecall_naive_bayes = get_recall(Y_test, Y_pred_naive_bayes)\nrecall_log_naive_bayes = get_recall(Y_test, Y_pred_log_naive_bayes)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(f\"The proportion of spam emails the standard Naive Bayes model can correctly classify as spam (recall) is: {recall_naive_bayes:.4f}\")\nprint(f\"The proportion of spam emails the log Naive Bayes model can correctly classify as spam (recall) is: {recall_log_naive_bayes:.4f}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "Ok, both models perform pretty well in **detecting spams**, being able to correctly identify 98% of them! This metric tells us about the model's **sensitivity**. In other words, this metric shows us how effective the model is in detecting a spam email.\n\nNow you are left with the second question. It is related to another metric called *[precision](https://en.wikipedia.org/wiki/Precision_and_recall)*. \n\nTo answer the question you must look at all emails the Naive Bayes models classify as spam and, in that pool, how many are **in fact** spam? This is an important metric to look, because a model that classifies any email as spam is a model that correctly classifies 100% of the spam emails, however it is pointless! Furthermore, you must avoid sending regular emails to the spam folder, otherwise the users may lose important emails.\n\nThis question is related to what it is called **false positives**. In other words, now you are looking at how many ham emails the algorithm sends to the spam folder. In the next code block, you will build a function to compute the false positives.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_false_positives(Y_true, Y_pred):\n    \"\"\"\n    Calculate the number of false positives instances in binary classification.\n\n    Parameters:\n    - Y_true (list): List of true labels (0 or 1) for each instance.\n    - Y_pred (list): List of predicted labels (0 or 1) for each instance.\n\n    Returns:\n    - int: Number of false positives, where true label is 0 and predicted label is 1.\n    \"\"\"\n    \n    # Both Y_true and Y_pred must match in length.\n    if len(Y_true) != len(Y_pred):\n        return \"Number of true labels and predict labels must match!\"\n    n = len(Y_true)\n\n    false_positives = 0\n    # Iterate over the number of elements in the list\n    for i in range(n):\n        # Get the true label for the considered email\n        true_label_i = Y_true[i]\n        # Get the predicted (model output) for the considered email\n        predicted_label_i = Y_pred[i]\n        # Increase the counter by 1 only if true_label_i = 0 and predicted_label_i = 0 (false positive)\n        if true_label_i == 0 and predicted_label_i == 1:\n            false_positives += 1\n    return false_positives",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "# Count the ham emails mistakenly labeled as spam (false positives). Let's use the function get_false_positives you've seen above\n \nfalse_positives_naive_bayes = get_false_positives(Y_test, Y_pred_naive_bayes)\nfalse_positives_log_naive_bayes = get_false_positives(Y_test, Y_pred_log_naive_bayes)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(f\"Number of false positives in the standard Naive Bayes model: {false_positives_naive_bayes}\")\nprint(f\"Number of false positives in the log Naive Bayes model: {false_positives_log_naive_bayes}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "This is a huge improvement! You went from 169 ham emails being mistakenly labeled as spam to only 4! To get a more meaningful number, you can compute the following quantity: \n\n- The proportion of actual spam emails (true positives) that exists in the pool of predicted spam emails. Note that the pool of predicted emails consist of **every spam email correctly labeled as spam** (true positives) and **every ham email mistakenly labeled as spam** (false positives).\n\nThis quantity is called **precision** and it is defined as:\n\n$$\\text{precision} = \\frac{\\text{true positives}}{\\text{true positives} + \\text{false positives}}$$\n\nThis metric tells you how **relevant** the output of your model is. As already discussed, a model that predicts every email as spam can correctly identify every spam email, however its output is irrelevant since it sends every ham email to the spam folder. You will now implement it.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_precision(Y_true, Y_pred):\n    \"\"\"\n    Calculate precision, a metric for the performance of a classification model,\n    by computing the ratio of true positives to the sum of true positives and false positives.\n\n    Parameters:\n    - Y_true (list): True labels.\n    - Y_pred (list): Predicted labels.\n\n    Returns:\n    - precision (float): Precision score.\n    \"\"\"\n    # Get the true positives\n    true_positives = get_true_positives(Y_true, Y_pred)\n    false_positives = get_false_positives(Y_true, Y_pred)\n    precision = true_positives/(true_positives + false_positives)\n    return precision",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "print(f\"Precision of the standard Naive Bayes model: {get_precision(Y_test, Y_pred_naive_bayes):.4f}\")\nprint(f\"Precision of the log Naive Bayes model: {get_precision(Y_test, Y_pred_log_naive_bayes):.4f}\")",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": "The first version of the model has a precision of 59.57%. In other words, from 100 emails the model classifies as spam, only around 60 of them are in fact spam. This means that this model would send 40 ham emails to the spam folder, indicating that, even though very sensitive, it is not very reliable. \n\nOn the other hand, the improved model has a precision of 98.42%! So from 100 emails classified as spam, only around 2 will be actually ham emails. A much more reliable output. ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Congratulations! You have completed the entire assignment and the appendix section! ",
      "metadata": {}
    }
  ]
}